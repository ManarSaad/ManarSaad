{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPa8mBXV4eQllUB9h1uQY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManarSaad/ManarSaad/blob/main/Assesment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hugging face agents is a framework that use the empower the LLM to do complex task like using logical thinking. Agent such as query agent in LLM is the tool that can perform advance tasks such as converting human request to a query form. <br>\n",
        "* Hugging face pipeline for text generation. are object that combine number of tasks, such as choosing the model task, model name, tokanizing model, in only one call.<br>\n",
        "* HF inference endpoints is a way of requesting a communication between a user to hugging face server, in order to use the feature of this platform<br>\n",
        "* LLM is a pweorful tool that can reduce the effort of making something such as creating image. Image generation models can now understand the human text promot to accordingly retreive images. It support many use cases such as Fashion Industry\n"
      ],
      "metadata": {
        "id": "PgBKglMkycGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Captions"
      ],
      "metadata": {
        "id": "jEC0hp1LDFSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ],
      "metadata": {
        "id": "Vy7T7fJu6gkP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gjzxi-gYxLQ6"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "\"https://c2.staticflickr.com/1/50/133067217_9c27664b97_o.jpg\", # river\n",
        "         \"https://farm7.staticflickr.com/4052/4241923013_b7c09bd53b_o.jpg\", # man walk in the snow\n",
        "         \"https://farm6.staticflickr.com/2686/4453538282_6e57bb0699_o.jpg\", # sunshine with the cloud\n",
        "         ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = [\n",
        "    Image.open(requests.get(url, stream=True).raw) for url in urls]"
      ],
      "metadata": {
        "id": "Qg0unju34TLj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load the BLIP processor and model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pJpYrM_S1NSL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_test=[]\n",
        "for image in image:\n",
        "  inputs = processor(images=image, return_tensors=\"pt\")\n",
        "  output_ids = model.generate(**inputs)\n",
        "  caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
        "  list_test.append(caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56KTqSR4-4xr",
        "outputId": "c7186430-3dbf-4ca8-9395-6f92d1e2bd04"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YBryfhpBHs0",
        "outputId": "cbdced7a-e56c-4ccc-94a9-299f255b54cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the river in the middle of the jungle',\n",
              " 'a person walking through a snowy forest',\n",
              " 'the sun is shining']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take User Caption Against My Images"
      ],
      "metadata": {
        "id": "4GwmDYJKDJIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modell = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processorr = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "JO1r76ox9mnj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Please enter a string: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avX6NEc_6oeD",
        "outputId": "7def0b6f-9542-48b9-d82b-eb4f82f9a6ae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter a string: Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input_test=\"Sky and sunshine\"\n"
      ],
      "metadata": {
        "id": "IjPZ6Oj0B-YI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputss = processorr(\n",
        "    text=list(user_input_test),\n",
        "    images=image,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "outputss = model(**inputss)\n",
        "\n",
        "probs = outputss.logits_per_image.argmax(dim=1)\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMxajOe_BvCq",
        "outputId": "f83ceb53-bbda-427b-fa39-165656ba645d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0544, 0.0157, 0.0672, 0.0912, 0.1299, 0.0191, 0.0702, 0.0912, 0.0544,\n",
              "         0.1233, 0.0191, 0.0544, 0.0453, 0.1100, 0.0191, 0.0355]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}